
## Guideline: How to Add a New Task

### Step 1: Create a Dataset

Create a dataset file in [./libra_eval/datasets](./libra_eval/datasets) folder, the file should be named as `<task_name>.jsonl`.
Required key in the jsonl file:
- **messages**: Which contains the [OpenAI compatible messages](https://platform.openai.com/docs/guides/chat-completions/getting-started). Each message should have a `role` and `content` field. The `role` can be `system`, `user`, or  `assistant`. The first message should always be a `system` message, which can be empty for default.
- Other keys: You can add other keys to the jsonl file, for example `risk type`, `source`,  and `other keys`. Try keep as much information as possible in the jsonl file.

Please refer to the notebook [./docs/format_data.ipynb](./docs/format_data.ipynb) for more information on how to format the data.

### Step 2: Create a Task Class

Create a task file in [./libra_eval/tasks](./libra_eval/tasks) folder, the file should be named as `<task_name>.py`.
Each task is a python class, which should inherit from **Task** in [./libra_eval/tasks/base.py](./libra_eval/tasks/base.py). We have implemented the **BaseDirectRiskyTask** for you. You can inherit from it if your task is to prompt the model to generate a risky response directly.

Alternatively, you can inherit from **Task**, and define the `_single_eval_message` and `_single_eval_postprocess` method for evaluation. The input field `instance` is the json object in the dataset file. 

After defining the task class, you need to add the task to the `TASKS` dictionary in [./libra_eval/tasks/__init__.py](./libra_eval/tasks/__init__.py).


### Step 3: Test

Run evaluation with your task:
```bash
python -m libra_eval.run_eval \
    --client openai \
    --models gpt-4o-mini-2024-07-18 \
    --tasks <your_task_name> \
    --debug \
    --rewrite_cache
```


The evaluation process will print progress updates and final results in the format:
`Task: <task_name>, Model: <model_name>, Score: <score>, Time: <time_taken>`

Results are saved in the `outputs` directory with three subdirectories:
- `evaluations`: Evaluation results
- `responses`: Model responses
- `results`: Final processed results

> **Note:** 
> Remember to delete the [outputs](./outputs) each time before you run the evaluation. Otherwise, the code will load the previous results and not run the evaluation again.

> ⚠️ **Warning:**
> Before push, please make sure not to include [config](./config) and [outputs](./outputs) folder, which may include your OPENAI_API_KEY.
