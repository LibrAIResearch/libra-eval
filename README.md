# LibrA-Eval

## Introduction

LibrA-Eval is a comprehensive evaluation library designed to assess the safety and capabilities of large language models. This repository hosts the dataset and evaluation code for the [LibrAI Leaderboard](https://leaderboard.librai.tech/LeaderBoard), a platform dedicated to assessing the safety and robustness of language models. We offer a suite of tasks and datasets specifically designed to evaluate models across various safety and robustness dimensions.


## Installation

Install from source:

```bash
git clone https://github.com/LibrAIResearch/libra-eval
cd libra-eval
pip install -e .
```

## Usage

Basic usage example:

```python
from libra_eval import evaluate_model

# Run evaluation
results = evaluate_model(model_name="your-model")
```

## Guideline: How to Add a New Task

### Step 1: Create a Dataset

Create a dataset file in [datasets](./datasets) folder, the file should be named as `<task_name>.jsonl`.
Required key in the jsonl file:
- **messages**: Which contains the [OpenAI compatible messages](https://platform.openai.com/docs/guides/chat-completions/getting-started). Each message should have a `role` and `content` field. The `role` can be `system`, `user`, or  `assistant`. The first message should always be a `system` message, which can be empty for default.
- Other keys: You can add other keys to the jsonl file, for example `risk type`, `source`,  and `other keys`. Try keep as much information as possible in the jsonl file.

Please refer to the notebook [tutorial](format_data.ipynb) for more information on how to format the data.

### Step 2: Create a Task Class

Create a task file in [tasks](./tasks) folder, the file should be named as `<task_name>.py`.
Each task is a python class, which should inherit from **Task** in [tasks/base.py](./tasks/base.py). We have implemented the **BaseDirectRiskyTask** for you. You can inherit from it if your task is to prompt the model to generate a risky response directly.

Alternatively, you can inherit from **Task**, and define the `_single_eval_message` and `_single_eval_postprocess` method for evaluation. The input field `instance` is the json object in the dataset file. 

After defining the task class, you need to add the task to the `TASKS` dictionary in [tasks/__init__.py](./tasks/__init__.py).

> **Note:**
> Please try to follow the evaluation method in the original paper. For example, some dataset use string matching, some use LLM-as-a-judge. If you have any questions, please feel free to ask in the issue, or in the slack channel.

### Step 3: Test

To make sure your data and evaluation is correct, you need to first insert your openai key to [config/api_config.json](./config/api_config.json).

**Environment:** Our code requires openai>=1.0.0, and pandas. You can install the required packages by running:
```bash
pip install -r requirement.txt
```

After that, run the following command:
```bash
python -m libra_eval.run_eval --client openai --model gpt-4o-mini-2024-07-18 --task <your_task_name> --debug --rewrite_cache
# Please note your_task_name is define in tasks/__init__.py and may different from the file name.
```

Usually, it will finish within 1 minute, and cost less than 0.1 USD. 

If you worry about the correctness of the framework and environment, you can run the following command to test DoNotAnswer task:
```bash
python -m libra_eval.run_eval --client openai --model gpt-4o-mini-2024-07-18 --task do_not_answer --debug --rewrite_cache
```

Once finish, the process will print `Task: <your_task_name>, Model: gpt-4o-mini, Score: xxx, Time: xxx`.
The results will be output to `outputs` folder. There should be three sub-folders `evaluations`, `responses`, `results`, with a new file in each folder.

> **Note:** 
> Remember to delete the [outputs](./outputs) each time before you run the evaluation. Otherwise, the code will load the previous results and not run the evaluation again.

> ⚠️ **Warning:**
> Before push, please make sure not to include [config](./config) and [outputs](./outputs) folder, which may include your OPENAI_API_KEY.

## License

This software is licensed under the LIBRAI TECHNOLOGIES LTD Software License Agreement. See LICENSE.md for full terms.

## Citation

If you use this framework in your research, please cite:

```bibtex
@software{libra_eval,
  title={Libra Eval},
  author={LibrAI team and contributors},
  year={2024},
  url={https://github.com/LibrAIResearch/libra-eval}
}
```

## Contact

- Email: team@librai.tech
- GitHub: https://github.com/LibrAIResearch/libra-eval

## Contributing

We welcome contributions! Please see our contributing guidelines for more details.

