# LibrA-Eval

## Introduction

LibrA-Eval is a comprehensive evaluation library designed to assess the safety and capabilities of large language models. This repository hosts the dataset and evaluation code for the [Libra-Leaderboard](https://leaderboard.librai.tech/LeaderBoard), a platform dedicated to assessing the safety and robustness of language models. We offer a suite of [tasks and datasets](docs/dataset_summary.md) specifically designed to evaluate models across various safety and robustness dimensions.


## Installation

```bash
git clone https://github.com/LibrAIResearch/libra-eval
cd libra-eval
pip install -e .
```

## Usage

Before running evaluations, you may need to set up your API configuration. See [Configuration](#configuration) for more details.

### Run a single task

```bash
python -m libra_eval.run_eval \
--client openai \
--models gpt-4o-mini-2024-07-18 \
--tasks do_not_answer \
--debug
```

Key parameters:
- `client`: The LLM provider (options: "openai", "local", "next")
- `models`: Model name or comma-separated list of models to evaluate (use "all" for all supported models)
- `tasks`: Task name or comma-separated list of tasks (use "all" for all tasks)
- `debug`: Enable debug mode
- `rewrite_cache`: Force rewrite of cached results
- `n_samples_per_task`: (Optional) Number of samples to evaluate per task
- `output_dir`: (Optional) Directory for output files (default: "./outputs")
- `mode`: Pipeline execution mode (default: "full")
  - "inference": Only generate model responses
  - "evaluation": Only run evaluation on existing responses
  - "full": Run complete pipeline

### Example of complex usage
Example usage with different modes:
```bash
python -m libra_eval.run_eval \
--client openai \
--models gpt-4o-mini-2024-07-18,gpt-4o-2024-08-06 \
--tasks do_not_answer,ruozhibench \
--mode full
```

> **Note:** 
> Remember to add `--rewrite_cache` to force rewrite the cached results. Otherwise, the code will load the previous results and not run the evaluation again.


### Results Collection
Results are saved in the `outputs` directory with three subdirectories:
- `evaluations`: Evaluation results
- `responses`: Model responses
- `results`: Final processed results


Collect and summarize evaluation results:
```bash
python -m libra_eval.info.results [options]
```
Options:
- `--output_dir`: Directory containing evaluation outputs (default: "./outputs")
- `--models`: Specific model to analyze or 'all' for all models
- `--tasks`: Specific task to analyze or 'all' for all tasks
- `--debug`: Enable debug mode
- `--n_samples_per_task`: Number of samples per task

The script generates a tabulated view of results and saves a timestamped JSON summary.

### Configuration

Before running evaluations, you need to set up your API configuration. Configure the `config/api_config.json` file with your API keys:

- **LIBRAI_API_KEY**: (Required) Get your key at [LibrAI Evaluators](https://evaluators.librai.tech/Evaluators/).
- **NEXT_API_KEY**: (Optional) Get your key at [OpenAI-Next](https://api.openai-next.com/login). You will only need this if you want to evaluate models hosted on OpenAI-Next.

```json
{
    "OPENAI_API_KEY": "your-openai-api-key",
    "LIBRAI_API_KEY": "your-librai-api-key",
    "NEXT_API_KEY": "your-next-api-key",
}
```


> ⚠️ **Important**: Never commit your API keys to version control. 


## Utility Scripts

The `info` directory contains utility scripts for querying and analyzing the datasets, models, and results:

**Available Models:** 
```bash
python -m libra_eval.info.models
```

**Available Tasks:** 
```bash
python -m libra_eval.info.tasks
```


> ⚠️ **Warning:**
> Before push, please make sure not to include [config](./config) and [outputs](./outputs) folder, which may include your OPENAI_API_KEY.

## License

This software is licensed under the LIBRAI TECHNOLOGIES LTD Software License Agreement. See LICENSE.md for full terms.

## Citation

If you use this framework in your research, please cite:

```bibtex
@misc{li2024libraleaderboardresponsibleaibalanced,
      title={Libra-Leaderboard: Towards Responsible AI through a Balanced Leaderboard of Safety and Capability},
      author={Haonan Li and Xudong Han and Zenan Zhai and Honglin Mu and Hao Wang and Zhenxuan Zhang and Yilin Geng and Shom Lin and Renxi Wang and Artem Shelmanov and Xiangyu Qi and Yuxia Wang and Donghai Hong and Youliang Yuan and Meng Chen and Haoqin Tu and Fajri Koto and Tatsuki Kuribayashi and Cong Zeng and Rishabh Bhardwaj and Bingchen Zhao and Yawen Duan and Yi Liu and Emad A. Alghamdi and Yaodong Yang and Yinpeng Dong and Soujanya Poria and Pengfei Liu and Zhengzhong Liu and Xuguang Ren and Eduard Hovy and Iryna Gurevych and Preslav Nakov and Monojit Choudhury and Timothy Baldwin},
      year={2024},
      eprint={2412.18551},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.18551},
}
```

## Contact

- Email: team@librai.tech
- GitHub: https://github.com/LibrAIResearch/libra-eval

## Contributing

We welcome contributions! Please see our contributing guidelines for more details.
