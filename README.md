# LibrA-Eval

## Introduction

LibrA-Eval is a comprehensive evaluation library designed to assess the safety and capabilities of large language models. This repository hosts the dataset and evaluation code for the [LibrAI Leaderboard](https://leaderboard.librai.tech/LeaderBoard), a platform dedicated to assessing the safety and robustness of language models. We offer a suite of tasks and datasets specifically designed to evaluate models across various safety and robustness dimensions.


## Installation

Install from source:

```bash
git clone https://github.com/LibrAIResearch/libra-eval
cd libra-eval
pip install -e .
```
## Configuration

Before running evaluations, you need to set up your API configuration. Configure the `config/api_config.json` file with your API keys:

```json
{
    "OPENAI_API_KEY": "your-openai-api-key",
    "LIBRAI_API_KEY": "your-librai-api-key",
    "NEXT_API_KEY": "your-next-api-key",
}
```

You'll need to obtain the following API keys:
1. **OPENAI_API_KEY**: Your OpenAI API key for accessing GPT models (required)
2. **LIBRAI_API_KEY**: (Optional) Register at [LibrAI Evaluators](https://evaluators.librai.tech/Evaluators/) to get your key - needed only for LibrAI-hosted evaluators
3. **NEXT_API_KEY**: (Optional) Available at [OpenAI-Next](https://api.openai-next.com/login) - needed only when evaluating models hosted on OpenAI-Next.

> ⚠️ **Important**: Never commit your API keys to version control. Make sure to add the `config` folder to your `.gitignore` file.

## Usage

Basic usage example:

```bash
python -m libra_eval.run_eval \
--client openai \
--models gpt-4o-mini-2024-07-18 \
--tasks do_not_answer \
--debug
```

Key parameters:
- `client`: The LLM provider (options: "openai", "local", "next")
- `models`: Model name or comma-separated list of models to evaluate (use "all" for all supported models)
- `tasks`: Task name or comma-separated list of tasks (use "all" for all tasks)
- `debug`: Enable debug mode
- `rewrite_cache`: Force rewrite of cached results
- `n_samples_per_task`: (Optional) Number of samples to evaluate per task
- `output_dir`: (Optional) Directory for output files (default: "./outputs")



## Utility Scripts

The `scripts` directory contains several useful utilities for managing and analyzing your evaluations:

### Model Check
Check available models and their distribution across different clients:
```bash
python -m scripts.check_models
```


### Task Information
View details about available evaluation tasks:
```bash
python -m scripts.check_tasks
```


### Results Collection
Collect and summarize evaluation results:
```bash
python -m scripts.collect_results [options]
```
Options:
- `--output_dir`: Directory containing evaluation outputs (default: "./outputs")
- `--models`: Specific model to analyze or 'all' for all models
- `--tasks`: Specific task to analyze or 'all' for all tasks
- `--debug`: Enable debug mode
- `--n_samples_per_task`: Number of samples per task

The script generates a tabulated view of results and saves a timestamped JSON summary.




## Guideline: How to Add a New Task

### Step 1: Create a Dataset

Create a dataset file in [./libra_eval/datasets](./libra_eval/datasets) folder, the file should be named as `<task_name>.jsonl`.
Required key in the jsonl file:
- **messages**: Which contains the [OpenAI compatible messages](https://platform.openai.com/docs/guides/chat-completions/getting-started). Each message should have a `role` and `content` field. The `role` can be `system`, `user`, or  `assistant`. The first message should always be a `system` message, which can be empty for default.
- Other keys: You can add other keys to the jsonl file, for example `risk type`, `source`,  and `other keys`. Try keep as much information as possible in the jsonl file.

Please refer to the notebook [./docs/format_data.ipynb](./docs/format_data.ipynb) for more information on how to format the data.

### Step 2: Create a Task Class

Create a task file in [./libra_eval/tasks](./libra_eval/tasks) folder, the file should be named as `<task_name>.py`.
Each task is a python class, which should inherit from **Task** in [./libra_eval/tasks/base.py](./libra_eval/tasks/base.py). We have implemented the **BaseDirectRiskyTask** for you. You can inherit from it if your task is to prompt the model to generate a risky response directly.

Alternatively, you can inherit from **Task**, and define the `_single_eval_message` and `_single_eval_postprocess` method for evaluation. The input field `instance` is the json object in the dataset file. 

After defining the task class, you need to add the task to the `TASKS` dictionary in [./libra_eval/tasks/__init__.py](./libra_eval/tasks/__init__.py).


### Step 3: Test

Run evaluation with your task:
```bash
python -m libra_eval.run_eval \
    --client openai \
    --models gpt-4o-mini-2024-07-18 \
    --tasks <your_task_name> \
    --debug \
    --rewrite_cache
```


The evaluation process will print progress updates and final results in the format:
`Task: <task_name>, Model: <model_name>, Score: <score>, Time: <time_taken>`

Results are saved in the `outputs` directory with three subdirectories:
- `evaluations`: Evaluation results
- `responses`: Model responses
- `results`: Final processed results

> **Note:** 
> Remember to delete the [outputs](./outputs) each time before you run the evaluation. Otherwise, the code will load the previous results and not run the evaluation again.

> ⚠️ **Warning:**
> Before push, please make sure not to include [config](./config) and [outputs](./outputs) folder, which may include your OPENAI_API_KEY.

## License

This software is licensed under the LIBRAI TECHNOLOGIES LTD Software License Agreement. See LICENSE.md for full terms.

## Citation

If you use this framework in your research, please cite:

```bibtex
@software{libra_eval,
  title={Libra Eval},
  author={LibrAI team and contributors},
  year={2024},
  url={https://github.com/LibrAIResearch/libra-eval}
}
```

## Contact

- Email: team@librai.tech
- GitHub: https://github.com/LibrAIResearch/libra-eval

## Contributing

We welcome contributions! Please see our contributing guidelines for more details.
