import os
import json
import time
from argparse import ArgumentParser

from libra_eval.tasks import TASKS
from libra_eval.llmclient import OpenAI_Client, Next_Client, Local_Client, LibrAI_Client, MODEL_LIST


def eval():
    parser = ArgumentParser()
    parser.add_argument("--client", type=str, default="openai")
    parser.add_argument("--models", type=str, default="gpt-3.5-turbo-0613", help="Comma-separated list of model names")
    parser.add_argument("--tasks", type=str, default="do_not_answer", help="Comma-separated list of task names")
    parser.add_argument("--output_dir", type=str, default="outputs")
    parser.add_argument("--n_samples_per_task", type=int, default=None)
    parser.add_argument("--debug", action="store_true")
    parser.add_argument("--rewrite_cache", action="store_true")
    args = parser.parse_args()
    print(args)

    # Init
    current_dir = os.path.dirname(os.path.abspath(__file__))
    with open(os.path.join(current_dir, "config/api_config.json"), "r") as f:
        api_config = json.load(f)

    args.output_dir = os.path.join(current_dir, args.output_dir)

    if not os.path.exists(args.output_dir):
        os.makedirs(args.output_dir)

    # Client of models to test
    if args.client == "openai":
        client_class = OpenAI_Client
    elif args.client == "local":
        client_class = Local_Client
    elif args.client == "next":
        client_class = Next_Client
    else:
        raise ValueError("Client not support.")

    # LLM-evaluator
    llm_eval_client = OpenAI_Client(
        model="gpt-4o-2024-08-06",
        api_config=api_config,
    )

    if args.tasks == "all":
        task_list = TASKS.keys()
    else:
        task_list = args.tasks.split(",")

    if args.models == "all":
        model_list = MODEL_LIST
    else:
        model_list = args.models.split(",")

    librai_client = LibrAI_Client(
        api_key = api_config["LIBRAI_API_KEY"]
    )

    print(f"In total {len(model_list)} models and {len(task_list)} tasks to evaluate.")
    for model in model_list:
        for task in task_list:
            try:
                st = time.time()
                print(f"Task: {task}, Model: {model}, evaluation started.")
                llm_client = client_class(model=model,api_config=api_config)
                task_instance = TASKS[task](debug=args.debug, output_dir=args.output_dir, n_samples_per_task=args.n_samples_per_task)
                score = task_instance.run_pipeline(llm_client=llm_client, llm_eval_client=llm_eval_client, librai_client=librai_client, rewrite_cache=args.rewrite_cache)
                et = time.time()
                print(f"Task: {task}, Model: {model}, evaluation done. Score: {score:.3f}, Time: {et - st:.2f}s")
            except Exception as e:
                print(f"Task: {task}, Model: {model}, Error: {e}. To debug, go to run_eval.py and uncomment the try-except block.")
                continue


if __name__ == "__main__":
    eval()
