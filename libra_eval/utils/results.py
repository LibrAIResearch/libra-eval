# scripts/collect_results.py
import os
import json
import datetime
from tabulate import tabulate
from argparse import ArgumentParser
from typing import Dict, List, Union, Optional

from libra_eval.tasks import TASKS
from libra_eval.llmclient import MODEL_LIST
from libra_eval.utils.logger import simple_logger as logger

def parse_args():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    default_output_dir = os.path.join(current_dir, "..", "outputs")
    
    parser = ArgumentParser(description="Collect and summarize evaluation results")
    parser.add_argument("--output_dir", type=str, default=default_output_dir,
                       help="Directory containing evaluation outputs")
    parser.add_argument("--models", type=str, default="all",
                       help="Specific model to analyze or 'all' for all models")
    parser.add_argument("--tasks", type=str, default="all",
                       help="Specific task to analyze or 'all' for all tasks")
    parser.add_argument("--debug", action="store_true",
                       help="Enable debug mode")
    parser.add_argument("--n_samples_per_task", type=int, default=None,
                       help="Number of samples per task")
    return parser.parse_args()

def get_models_and_tasks(args) -> tuple[List[str], List[str]]:
    """Get list of models and tasks based on arguments."""
    models = [args.models] if args.models != "all" else MODEL_LIST
    models = [m.split("/")[-1] for m in models]  # Clean model names
    
    tasks = [args.tasks] if args.tasks != "all" else TASKS.keys()
           
    return models, list(tasks)

def get_result_path(output_dir: str, task_name: str, model_name: str, 
                    debug: bool, n_samples: Optional[int]) -> str:
    """Construct the result file path."""
    if debug:
        task_name = f"debug_{task_name}"
    elif n_samples is not None:
        task_name = f"{task_name}_{n_samples}"
    
    return os.path.join(output_dir, "results", f"{task_name}_{model_name}.json")

def load_result(result_path: str) -> tuple[Union[float, str], Union[float, str]]:
    """Load and process a single result file."""
    try:
        with open(result_path, "r") as f:
            result = json.load(f)
            score = result["score"]
            return f"{score*100:.0f}", score
    except FileNotFoundError:
        logger.warning(f"Missing result file: {result_path}")
        return "N/A", "N/A"

def collect_results(args) -> tuple[List[List], Dict, List[str]]:
    """Collect all results and prepare for display."""
    models, tasks = get_models_and_tasks(args)
    
    summary: Dict[str, Dict[str, Union[float, str]]] = {}
    all_results = []
    headers = ["Model"] + [n[-8:] for n in tasks]  # Truncate long task names
    
    for model in models:
        scores = {}
        row = [model]
        
        for task in tasks:
            result_path = get_result_path(args.output_dir, task, model, 
                                        args.debug, args.n_samples_per_task)
            
            display_score, actual_score = load_result(result_path)
            row.append(display_score)
            scores[task] = actual_score
            
        all_results.append(row)
        summary[model] = scores
        
    return all_results, summary, headers

def save_summary(summary: Dict, output_dir: str):
    """Save summary to a timestamped JSON file."""
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    output_path = os.path.join(output_dir, f"summary_{timestamp}.json")
    
    with open(output_path, "w") as f:
        json.dump(summary, f, indent=2)

def main():
    args = parse_args()
    
    # Collect and process results
    all_results, summary, headers = collect_results(args)
    
    # Display results table
    logger.info(tabulate(all_results, headers, tablefmt="pipe", stralign="left"))
    
    # Save summary
    save_summary(summary, args.output_dir)

if __name__ == "__main__":
    main()