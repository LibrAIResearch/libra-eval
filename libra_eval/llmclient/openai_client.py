import time
import traceback
from openai import OpenAI
from .base import MODEL_LIST, BaseClient

models = list(filter(lambda x: x.startswith("gpt") or x.startswith("o"), MODEL_LIST))
name_mapping = {i:i for i in models}


class OpenAI_Client(BaseClient):
    def __init__(
        self,
        model: str = "gpt-4-turbo",
        api_config: dict = None,
        max_requests_per_minute=200,
        request_window=60,
        for_eval=False, # Separate key for evaluator
        client_name="OpenAI",
    ):
        super().__init__(model, api_config, max_requests_per_minute, request_window)
        self.client_name = client_name
        if self.client_name == "OpenAI":
            assert self.api_config["OPENAI_API_KEY"] != ""
            api_key_name = "OPENAI_API_KEY" if not for_eval or "OPENAI_API_KEY_FOR_EVAL" not in self.api_config else "OPENAI_API_KEY_FOR_EVAL"
            self.client = OpenAI(api_key=self.api_config[api_key_name])
            self.name_mapping = name_mapping

    def _call(self, messages: str, **kwargs):
        if "seed" not in kwargs:
            kwargs["seed"] = 42  # default seed is 42

        # Convert system role to developer if needed
        if self.system_role_name == "developer":
            for msg in messages:
                if msg["role"] == "system":
                    msg["role"] = "developer"

        num_retries = kwargs.pop("num_retries", 1)
        post_check_function = kwargs.pop("post_check_function", None)

        r = ""

        for i in range(num_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.name_mapping[self.model],
                    messages=messages,
                    **kwargs  # passing the kwargs, such as temperature, max_tokens, response_format, etc, to the OpenAI API.
                )
                r = response.choices[0].message.content
                if post_check_function is None:
                    break
                else:
                    post_r = post_check_function(r)
                    if post_r:
                        return post_r
                    else:
                        print(f"Warning: Post check function failed. Response is {r}. Retrying {i} ...")
            except Exception as e:
                print("Error in LLM Client: ")
                print(traceback.format_exc())  # This will print the full stack trace
                time.sleep(1)      

        if r == "":
            print(f"{self.client_name} Client Warning: Empty response from LLM Client call.")
        
        return r

    def _log_usage(self, usage_dict):
        try:
            self.usage.prompt_tokens += usage_dict.prompt_tokens
            self.usage.completion_tokens += usage_dict.completion_tokens
        except:  # noqa E722
            print("Warning: prompt_tokens or completion_token not found in usage_dict")


    def construct_message_list(
        self,
        prompt_list: list[str],
        system_role: str = "You are a helpful assistant designed to output JSON.",
        no_system_role: bool = False,
    ):
        messages_list = list()
        for prompt in prompt_list:
            if no_system_role:
                messages = [
                    {"role": "user", "content": prompt},
                ]
            else:
                messages = [
                    {"role": self.system_role_name, "content": system_role},
                    {"role": "user", "content": prompt},
                ]
            messages_list.append(messages)
        return messages_list
